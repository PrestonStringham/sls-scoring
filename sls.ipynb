{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "719597c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "tf.config.optimizer.set_jit(True)\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87156a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5ba7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_score_df(path):\n",
    "    files = [f for f in glob.glob(path + \"*\")]\n",
    "    scores = []\n",
    "    for i in files:\n",
    "        scores.append(float(i[5:8]))\n",
    "    return pd.DataFrame(files,scores).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b95b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = files_to_score_df('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69b89bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.1</td>\n",
       "      <td>data/9.1-SLSSC2019.mov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1</td>\n",
       "      <td>data/9.1-SLSSC2019.avi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                       0\n",
       "0    9.1  data/9.1-SLSSC2019.mov\n",
       "1    9.1  data/9.1-SLSSC2019.avi"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c68b50",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/92/_804j2_s6cb9ytq1q1j27h500000gn/T/ipykernel_11633/3539040688.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# converting to gray-scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# write to gray-scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.5) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "source = cv2.VideoCapture(score_df.iloc[0][0])\n",
    "# We need to set resolutions. \n",
    "# so, convert them from float to integer. \n",
    "frame_width = int(source.get(3)) \n",
    "frame_height = int(source.get(4)) \n",
    "   \n",
    "size = (frame_width, frame_height) \n",
    "\n",
    "result = cv2.VideoWriter('gray.avi',  \n",
    "            cv2.VideoWriter_fourcc(*'MJPG'), \n",
    "            30, size, 0) \n",
    "  \n",
    "# running the loop \n",
    "while True: \n",
    "  \n",
    "    # extracting the frames \n",
    "    ret, img = source.read() \n",
    "      \n",
    "    # converting to gray-scale \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "    # write to gray-scale \n",
    "    result.write(gray)\n",
    "\n",
    "    # displaying the video \n",
    "    cv2.imshow(\"Live\", gray) \n",
    "  \n",
    "    # exiting the loop \n",
    "    key = cv2.waitKey(1) \n",
    "    if key == ord(\"q\"): \n",
    "        break\n",
    "# closing the window \n",
    "cv2.destroyAllWindows() \n",
    "source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7a2103c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/9.1-SLSSC2019.avi'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df.iloc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fccf9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    # Empty List declared to store video frames\n",
    "    frames_list = []\n",
    "    \n",
    "    # Reading the Video File Using the VideoCapture\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Iterating through Video Frames\n",
    "    while True:\n",
    "\n",
    "        # Reading a frame from the video file \n",
    "        success, frame = video_reader.read() \n",
    "    \n",
    "        # If Video frame was not successfully read then break the loop\n",
    "        if not success:\n",
    "            break\n",
    "            \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
    "        normalized_frame = gray / 255\n",
    "        \n",
    "        # Appending the normalized frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    "    \n",
    "    # Closing the VideoCapture object and releasing all resources. \n",
    "    video_reader.release()\n",
    "\n",
    "    # returning the frames list \n",
    "    return frames_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "475210f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame([[frames_extraction(score_df.iloc[1][0])]], np.array([score_df.iloc[1]['index']])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c3c3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df.iloc[0][0]\n",
    "y = test_df.iloc[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb197e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [X]\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7946def0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.09803922, 0.09803922, 0.09803922, ..., 0.15294118,\n",
       "          0.15294118, 0.15294118],\n",
       "         [0.22745098, 0.22745098, 0.22745098, ..., 0.15294118,\n",
       "          0.15294118, 0.15294118],\n",
       "         [0.09803922, 0.09803922, 0.09803922, ..., 0.14509804,\n",
       "          0.14509804, 0.14509804],\n",
       "         ...,\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ],\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ],\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        [[0.0627451 , 0.0627451 , 0.0627451 , ..., 0.17647059,\n",
       "          0.17647059, 0.17647059],\n",
       "         [0.10588235, 0.10588235, 0.10588235, ..., 0.1372549 ,\n",
       "          0.1372549 , 0.1372549 ],\n",
       "         [0.0627451 , 0.0627451 , 0.0627451 , ..., 0.18823529,\n",
       "          0.18823529, 0.18823529],\n",
       "         ...,\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ],\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ],\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        [[0.08235294, 0.08235294, 0.08235294, ..., 0.1254902 ,\n",
       "          0.12156863, 0.12156863],\n",
       "         [0.0745098 , 0.0745098 , 0.0745098 , ..., 0.18823529,\n",
       "          0.18823529, 0.18823529],\n",
       "         [0.07843137, 0.07843137, 0.07843137, ..., 0.12941176,\n",
       "          0.1254902 , 0.1254902 ],\n",
       "         ...,\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ],\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ],\n",
       "         [0.15294118, 0.15294118, 0.15294118, ..., 0.7372549 ,\n",
       "          0.7372549 , 0.7372549 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.11764706, 0.12156863, 0.1254902 , ..., 0.25098039,\n",
       "          0.26666667, 0.27843137],\n",
       "         [0.07843137, 0.07843137, 0.08235294, ..., 0.35294118,\n",
       "          0.3372549 , 0.32941176],\n",
       "         [0.10588235, 0.11372549, 0.12156863, ..., 0.21568627,\n",
       "          0.23137255, 0.23921569],\n",
       "         ...,\n",
       "         [0.68627451, 0.68627451, 0.68627451, ..., 0.40392157,\n",
       "          0.40392157, 0.40392157],\n",
       "         [0.68627451, 0.68627451, 0.68627451, ..., 0.40392157,\n",
       "          0.40392157, 0.40392157],\n",
       "         [0.68627451, 0.68627451, 0.68627451, ..., 0.40392157,\n",
       "          0.40392157, 0.40392157]],\n",
       "\n",
       "        [[0.16078431, 0.16078431, 0.16078431, ..., 0.34117647,\n",
       "          0.33333333, 0.3254902 ],\n",
       "         [0.17647059, 0.18431373, 0.19215686, ..., 0.36470588,\n",
       "          0.35686275, 0.35294118],\n",
       "         [0.20392157, 0.20392157, 0.2       , ..., 0.3372549 ,\n",
       "          0.33333333, 0.32941176],\n",
       "         ...,\n",
       "         [0.68627451, 0.68627451, 0.68627451, ..., 0.6627451 ,\n",
       "          0.6627451 , 0.6627451 ],\n",
       "         [0.68627451, 0.68627451, 0.68627451, ..., 0.6627451 ,\n",
       "          0.6627451 , 0.6627451 ],\n",
       "         [0.68627451, 0.68627451, 0.68627451, ..., 0.6627451 ,\n",
       "          0.6627451 , 0.6627451 ]],\n",
       "\n",
       "        [[0.08235294, 0.08235294, 0.08235294, ..., 0.10980392,\n",
       "          0.09803922, 0.09019608],\n",
       "         [0.18823529, 0.18431373, 0.18039216, ..., 0.3372549 ,\n",
       "          0.33333333, 0.33333333],\n",
       "         [0.07843137, 0.07843137, 0.08235294, ..., 0.10980392,\n",
       "          0.09803922, 0.09019608],\n",
       "         ...,\n",
       "         [0.69411765, 0.69411765, 0.69411765, ..., 0.6627451 ,\n",
       "          0.6627451 , 0.6627451 ],\n",
       "         [0.69411765, 0.69411765, 0.69411765, ..., 0.6627451 ,\n",
       "          0.6627451 , 0.6627451 ],\n",
       "         [0.69411765, 0.69411765, 0.69411765, ..., 0.6627451 ,\n",
       "          0.6627451 , 0.6627451 ]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bcc90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [y]\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1b9f0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26aee31f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (1, 1) was passed for an output of shape (None, 59, 799, 1) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-65002701f29e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/sls/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sls/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sls/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sls/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sls/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sls/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (1, 1) was passed for an output of shape (None, 59, 799, 1) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(60, kernel_size=(2,2), input_shape=(1, 60, 800, 800)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c91af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
